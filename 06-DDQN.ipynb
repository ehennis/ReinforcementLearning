{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2016, Google DeepMind ([Link](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847)) decided to alter the DQN algorithm the same way the original Q-Learner algorithm was updated by adding a second network. The team found that the overestimation that affected the q-learner also affected the DQN algoritm. When the original double q-learner was introduced they proved that it worked in that setting and in this paper they prove it can be generalized to work with large scale function approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Summary**  \n",
    "This algorithm combines the benefits of the Double Q-Learner as the benefits of deep learning. We get the function approximation so that we can have a continuous state space plus we keep from having the overestimation from Q-Learning. This document will be a combination of the DQN and Double Q-Learner so it will be mostly review. But, this algorithm is so much more powerful that you should explore the other gyms at OpenAI and see what you can solve.  \n",
    "\n",
    "One thing to note, we need to update the weights of the second neural network with the weights from the first neural network. We didn't do this in the Double Q-Learner since the tables were both getting updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CartPole Example**  \n",
    "Again we will use the [CartPole](https://gym.openai.com/envs/CartPole-v1/) environment from OpenAI.  \n",
    "\n",
    "The actions are 0 to push the cart to the left and 1 to push the cart to the right.  \n",
    "\n",
    "The continuous state space is an X coordinate for location, the velocity of the cart, the angle of the pole, and the velocity at the tip of the pole. The X coordinate goes from -4.8 to +4.8, velocity is -Inf to +Inf, angle of the pole goes from -24 degrees to +24 degrees, tip velocity is -Inf to +Inf. With all of the possible combinations you can see why we can't create a Q table for each one.  \n",
    "\n",
    "To \"solve\" this puzzle you have to have an average reward of > 195 over 100 consecutive episodes. One thing to note, I am hard capping the rewards at 210 so this number can't average above that and it also could potentially drive the average down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[2018-11-15 21:11:21,037] Making new env: CartPole-v1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[50]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Imports and gym creation\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import random\n",
    "\n",
    "#Create Gym\n",
    "from gym import wrappers\n",
    "envCartPole = gym.make('CartPole-v1')\n",
    "envCartPole.seed(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPISODES = 500\n",
    "TRAIN_END = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_rate(): #Gamma\n",
    "    return 0.95\n",
    "\n",
    "def learning_rate(): #Alpha\n",
    "    return 0.001\n",
    "\n",
    "def batch_size():\n",
    "    return 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Double Deep Q-Network Class**  \n",
    "This class is the same as the DQN class from the last notebook with a few exceptions.  \n",
    "**init**:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We create a second NN for the target network  \n",
    "\n",
    "**update_target_from_model(self)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This class updates the weights of the target NN from the model NN\n",
    "\n",
    "**build_model(self)**:  \n",
    "**action(self,state)**:  \n",
    "**test_action(self,state)**:  \n",
    "**store(self, state, action, reward, nstate, done)**:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Same  \n",
    "\n",
    "**experience_replay(self, batch_size)**:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This class has the Double DQN changes. We grab the prediction targets from the target NN and then use that in the Q update rule.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DoubleDeepQNetwork():\n",
    "    def __init__(self, states, actions, alpha, gamma, epsilon,epsilon_min, epsilon_decay):\n",
    "        self.nS = states\n",
    "        self.nA = actions\n",
    "        self.memory = deque([], maxlen=2500)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        #Explore/Exploit\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.model = self.build_model()\n",
    "        self.model_target = self.build_model() #Second (target) neural network\n",
    "        self.update_target_from_model() #Update weights\n",
    "        self.loss = []\n",
    "        \n",
    "    def build_model(self):\n",
    "        model = Sequential() #linear stack of layers https://keras.io/models/sequential/\n",
    "        model.add(Dense(24, input_dim=self.nS, activation='relu')) #[Input] -> Layer 1\n",
    "        #   Dense: Densely connected layer https://keras.io/layers/core/\n",
    "        #   24: Number of neurons\n",
    "        #   input_dim: Number of input variables\n",
    "        #   activation: Rectified Linear Unit (relu) ranges >= 0\n",
    "        model.add(Dense(24, activation='relu')) #Layer 2 -> 3\n",
    "        model.add(Dense(self.nA, activation='linear')) #Layer 3 -> [output]\n",
    "        #   Size has to match the output (different actions)\n",
    "        #   Linear activation on the last layer\n",
    "        model.compile(loss='mean_squared_error', #Loss function: Mean Squared Error\n",
    "                      optimizer=Adam(lr=self.alpha)) #Optimaizer: Adam (Feel free to check other options)\n",
    "        return model\n",
    "\n",
    "    def update_target_from_model(self):\n",
    "        #Update the target model from the base model\n",
    "        self.model_target.set_weights( self.model.get_weights() )\n",
    "\n",
    "    def action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.nA) #Explore\n",
    "        action_vals = self.model.predict(state) #Exploit: Use the NN to predict the correct action from this state\n",
    "        return np.argmax(action_vals[0])\n",
    "\n",
    "    def test_action(self, state): #Exploit\n",
    "        action_vals = self.model.predict(state)\n",
    "        return np.argmax(action_vals[0])\n",
    "\n",
    "    def store(self, state, action, reward, nstate, done):\n",
    "        #Store the experience in memory\n",
    "        self.memory.append( (state, action, reward, nstate, done) )\n",
    "\n",
    "    def experience_replay(self, batch_size):\n",
    "        #Execute the experience replay\n",
    "        minibatch = random.sample( self.memory, batch_size ) #Randomly sample from memory\n",
    "\n",
    "        #Convert to numpy for speed by vectorization\n",
    "        x = []\n",
    "        y = []\n",
    "        np_array = np.array(minibatch)\n",
    "        st = np.zeros((0,self.nS)) #States\n",
    "        nst = np.zeros( (0,self.nS) )#Next States\n",
    "        for i in range(len(np_array)): #Creating the state and next state np arrays\n",
    "            st = np.append( st, np_array[i,0], axis=0)\n",
    "            nst = np.append( nst, np_array[i,3], axis=0)\n",
    "        st_predict = self.model.predict(st) #Here is the speedup! I can predict on the ENTIRE batch\n",
    "        nst_predict = self.model.predict(nst)\n",
    "        nst_predict_target = self.model_target.predict(nst) #Predict from the TARGET\n",
    "        index = 0\n",
    "        for state, action, reward, nstate, done in minibatch:\n",
    "            x.append(state)\n",
    "            #Predict from state\n",
    "            nst_action_predict_target = nst_predict_target[index]\n",
    "            nst_action_predict_model = nst_predict[index]\n",
    "            if done == True: #Terminal: Just assign reward much like {* (not done) - QB[state][action]}\n",
    "                target = reward\n",
    "            else:   #Non terminal\n",
    "                target = reward + self.gamma * nst_action_predict_target[np.argmax(nst_action_predict_model)] #Using Q to get T is Double DQN\n",
    "            target_f = st_predict[index]\n",
    "            target_f[action] = target\n",
    "            y.append(target_f)\n",
    "            index += 1\n",
    "        #Reshape for Keras Fit\n",
    "        x_reshape = np.array(x).reshape(batch_size,self.nS)\n",
    "        y_reshape = np.array(y)\n",
    "        epoch_count = 1\n",
    "        hist = self.model.fit(x_reshape, y_reshape, epochs=epoch_count, verbose=0)\n",
    "        #Graph Losses\n",
    "        for i in range(epoch_count):\n",
    "            self.loss.append( hist.history['loss'][i] )\n",
    "        #Decay Epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the agents\n",
    "nS = envCartPole.observation_space.shape[0] #This is only 4\n",
    "nA = envCartPole.action_space.n #Actions\n",
    "dqn = DoubleDeepQNetwork(nS, nA, learning_rate(), discount_rate(), 1, 0.001, 0.995 )\n",
    "\n",
    "batch_size = batch_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/500, score: 24.0, e: 1\n",
      "episode: 1/500, score: 19.0, e: 0.9137248860125932\n",
      "episode: 2/500, score: 13.0, e: 0.8603841919146962\n",
      "episode: 3/500, score: 12.0, e: 0.8142285204175609\n",
      "episode: 4/500, score: 10.0, e: 0.778312557068642\n",
      "episode: 5/500, score: 17.0, e: 0.7183288830986236\n",
      "episode: 6/500, score: 9.0, e: 0.6900935609921609\n",
      "episode: 7/500, score: 11.0, e: 0.6563549768288433\n",
      "episode: 8/500, score: 16.0, e: 0.6088145090359074\n",
      "episode: 9/500, score: 16.0, e: 0.5647174463480732\n",
      "episode: 10/500, score: 10.0, e: 0.5398075216808175\n",
      "episode: 11/500, score: 25.0, e: 0.47862223409330756\n",
      "episode: 12/500, score: 11.0, e: 0.45522245551230495\n",
      "episode: 13/500, score: 17.0, e: 0.42013897252428334\n",
      "episode: 14/500, score: 8.0, e: 0.40565285250151817\n",
      "episode: 15/500, score: 16.0, e: 0.37627099809304654\n",
      "episode: 16/500, score: 10.0, e: 0.3596735257153405\n",
      "episode: 17/500, score: 11.0, e: 0.3420891339682016\n",
      "episode: 18/500, score: 12.0, e: 0.3237376186352221\n",
      "episode: 19/500, score: 11.0, e: 0.3079101286968243\n",
      "episode: 20/500, score: 11.0, e: 0.29285644267656924\n",
      "episode: 21/500, score: 9.0, e: 0.28134514724562876\n",
      "episode: 22/500, score: 14.0, e: 0.26359640222486147\n",
      "episode: 23/500, score: 19.0, e: 0.2408545925762412\n",
      "episode: 24/500, score: 18.0, e: 0.2211807388415433\n",
      "episode: 25/500, score: 13.0, e: 0.20826882814336947\n",
      "episode: 26/500, score: 14.0, e: 0.19513012515638165\n",
      "episode: 27/500, score: 17.0, e: 0.1800916657318127\n",
      "episode: 28/500, score: 50.0, e: 0.14087196468590776\n",
      "episode: 29/500, score: 12.0, e: 0.13331482894782642\n",
      "episode: 30/500, score: 18.0, e: 0.12242520289863423\n",
      "episode: 31/500, score: 10.0, e: 0.11702497557911415\n",
      "episode: 32/500, score: 23.0, e: 0.10480604571960442\n",
      "episode: 33/500, score: 18.0, e: 0.09624511776741324\n",
      "episode: 34/500, score: 17.0, e: 0.0888276147181114\n",
      "episode: 35/500, score: 16.0, e: 0.08239373898667031\n",
      "episode: 36/500, score: 10.0, e: 0.07875931641927113\n",
      "episode: 37/500, score: 36.0, e: 0.0660860453679829\n",
      "episode: 38/500, score: 88.0, e: 0.04272851397543923\n",
      "episode: 39/500, score: 19.0, e: 0.03904210656169572\n",
      "episode: 40/500, score: 18.0, e: 0.03585300941485119\n",
      "episode: 41/500, score: 20.0, e: 0.032595988006089364\n",
      "episode: 42/500, score: 20.0, e: 0.029634846598205186\n",
      "episode: 43/500, score: 15.0, e: 0.027626496583102015\n",
      "episode: 44/500, score: 18.0, e: 0.02536986677519817\n",
      "episode: 45/500, score: 16.0, e: 0.023532301163728918\n",
      "episode: 46/500, score: 53.0, e: 0.018132788524664028\n",
      "episode: 47/500, score: 48.0, e: 0.014326806379480736\n",
      "episode: 48/500, score: 91.0, e: 0.009124869429963996\n",
      "episode: 49/500, score: 51.0, e: 0.0071020004589527575\n",
      "episode: 50/500, score: 24.0, e: 0.006328656923128\n",
      "episode: 51/500, score: 23.0, e: 0.005667862809171332\n",
      "episode: 52/500, score: 29.0, e: 0.004925673152649918\n",
      "episode: 53/500, score: 195.0, e: 0.0018627014607313632\n",
      "episode: 54/500, score: 210.0, e: 0.0009954703940636294\n",
      "episode: 55/500, score: 134.0, e: 0.0009954703940636294\n",
      "episode: 56/500, score: 67.0, e: 0.0009954703940636294\n",
      "episode: 57/500, score: 143.0, e: 0.0009954703940636294\n",
      "episode: 58/500, score: 210.0, e: 0.0009954703940636294\n",
      "episode: 59/500, score: 175.0, e: 0.0009954703940636294\n",
      "episode: 60/500, score: 180.0, e: 0.0009954703940636294\n",
      "episode: 61/500, score: 210.0, e: 0.0009954703940636294\n",
      "episode: 62/500, score: 130.0, e: 0.0009954703940636294\n",
      "episode: 63/500, score: 177.0, e: 0.0009954703940636294\n",
      "episode: 64/500, score: 210.0, e: 0.0009954703940636294\n",
      "episode: 65/500, score: 210.0, e: 0.0009954703940636294\n",
      "episode: 66/500, score: 210.0, e: 0.0009954703940636294\n",
      "episode: 67/500, score: 203.0, e: 0.0009954703940636294\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "rewards = [] #Store rewards for graphing\n",
    "epsilons = [] # Store the Explore/Exploit\n",
    "TEST_Episodes = 0\n",
    "for e in range(EPISODES):\n",
    "    state = envCartPole.reset()\n",
    "    state = np.reshape(state, [1, nS]) # Resize to store in memory to pass to .predict\n",
    "    tot_rewards = 0\n",
    "    for time in range(210): #200 is when you \"solve\" the game. This can continue forever as far as I know\n",
    "        action = dqn.action(state)\n",
    "        nstate, reward, done, _ = envCartPole.step(action)\n",
    "        nstate = np.reshape(nstate, [1, nS])\n",
    "        tot_rewards += reward\n",
    "        dqn.store(state, action, reward, nstate, done) # Resize to store in memory to pass to .predict\n",
    "        state = nstate\n",
    "        #done: CartPole fell. \n",
    "        #time == 209: CartPole stayed upright\n",
    "        if done or time == 209:\n",
    "            rewards.append(tot_rewards)\n",
    "            epsilons.append(dqn.epsilon)\n",
    "            print(\"episode: {}/{}, score: {}, e: {}\"\n",
    "                  .format(e, EPISODES, tot_rewards, dqn.epsilon))\n",
    "            break\n",
    "        #Experience Replay\n",
    "        if len(dqn.memory) > batch_size:\n",
    "            dqn.experience_replay(batch_size)\n",
    "    #Update the weights after each episode (You can configure this for x steps as well\n",
    "    dqn.update_target_from_model()\n",
    "    #If our current NN passes we are done\n",
    "    #I am going to use the last 5 runs\n",
    "    if len(rewards) > 5 and np.average(rewards[-5:]) > 195:\n",
    "        #Set the rest of the EPISODES for testing\n",
    "        TEST_Episodes = EPISODES - e\n",
    "        TRAIN_END = e\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Testing started...\n",
      "episode: 0/433, score: 210.0, e: 0\n",
      "episode: 1/433, score: 210.0, e: 0\n",
      "episode: 2/433, score: 210.0, e: 0\n",
      "episode: 3/433, score: 210.0, e: 0\n",
      "episode: 4/433, score: 210.0, e: 0\n",
      "episode: 5/433, score: 210.0, e: 0\n",
      "episode: 6/433, score: 210.0, e: 0\n",
      "episode: 7/433, score: 210.0, e: 0\n",
      "episode: 8/433, score: 210.0, e: 0\n",
      "episode: 9/433, score: 210.0, e: 0\n",
      "episode: 10/433, score: 210.0, e: 0\n",
      "episode: 11/433, score: 210.0, e: 0\n",
      "episode: 12/433, score: 210.0, e: 0\n",
      "episode: 13/433, score: 210.0, e: 0\n",
      "episode: 14/433, score: 210.0, e: 0\n",
      "episode: 15/433, score: 210.0, e: 0\n",
      "episode: 16/433, score: 210.0, e: 0\n",
      "episode: 17/433, score: 210.0, e: 0\n",
      "episode: 18/433, score: 210.0, e: 0\n",
      "episode: 19/433, score: 210.0, e: 0\n",
      "episode: 20/433, score: 210.0, e: 0\n",
      "episode: 21/433, score: 210.0, e: 0\n",
      "episode: 22/433, score: 210.0, e: 0\n",
      "episode: 23/433, score: 210.0, e: 0\n",
      "episode: 24/433, score: 210.0, e: 0\n",
      "episode: 25/433, score: 210.0, e: 0\n",
      "episode: 26/433, score: 210.0, e: 0\n",
      "episode: 27/433, score: 210.0, e: 0\n",
      "episode: 28/433, score: 210.0, e: 0\n",
      "episode: 29/433, score: 197.0, e: 0\n",
      "episode: 30/433, score: 210.0, e: 0\n",
      "episode: 31/433, score: 210.0, e: 0\n",
      "episode: 32/433, score: 210.0, e: 0\n",
      "episode: 33/433, score: 210.0, e: 0\n",
      "episode: 34/433, score: 210.0, e: 0\n",
      "episode: 35/433, score: 210.0, e: 0\n",
      "episode: 36/433, score: 210.0, e: 0\n",
      "episode: 37/433, score: 210.0, e: 0\n",
      "episode: 38/433, score: 210.0, e: 0\n",
      "episode: 39/433, score: 210.0, e: 0\n",
      "episode: 40/433, score: 210.0, e: 0\n",
      "episode: 41/433, score: 210.0, e: 0\n",
      "episode: 42/433, score: 210.0, e: 0\n",
      "episode: 43/433, score: 210.0, e: 0\n",
      "episode: 44/433, score: 210.0, e: 0\n",
      "episode: 45/433, score: 210.0, e: 0\n",
      "episode: 46/433, score: 210.0, e: 0\n",
      "episode: 47/433, score: 210.0, e: 0\n",
      "episode: 48/433, score: 210.0, e: 0\n",
      "episode: 49/433, score: 210.0, e: 0\n",
      "episode: 50/433, score: 210.0, e: 0\n",
      "episode: 51/433, score: 210.0, e: 0\n",
      "episode: 52/433, score: 210.0, e: 0\n",
      "episode: 53/433, score: 210.0, e: 0\n",
      "episode: 54/433, score: 210.0, e: 0\n",
      "episode: 55/433, score: 210.0, e: 0\n",
      "episode: 56/433, score: 210.0, e: 0\n",
      "episode: 57/433, score: 210.0, e: 0\n",
      "episode: 58/433, score: 210.0, e: 0\n",
      "episode: 59/433, score: 210.0, e: 0\n",
      "episode: 60/433, score: 210.0, e: 0\n",
      "episode: 61/433, score: 210.0, e: 0\n",
      "episode: 62/433, score: 210.0, e: 0\n",
      "episode: 63/433, score: 210.0, e: 0\n",
      "episode: 64/433, score: 210.0, e: 0\n",
      "episode: 65/433, score: 210.0, e: 0\n",
      "episode: 66/433, score: 210.0, e: 0\n",
      "episode: 67/433, score: 210.0, e: 0\n",
      "episode: 68/433, score: 210.0, e: 0\n",
      "episode: 69/433, score: 210.0, e: 0\n",
      "episode: 70/433, score: 210.0, e: 0\n",
      "episode: 71/433, score: 210.0, e: 0\n",
      "episode: 72/433, score: 210.0, e: 0\n",
      "episode: 73/433, score: 210.0, e: 0\n",
      "episode: 74/433, score: 210.0, e: 0\n",
      "episode: 75/433, score: 210.0, e: 0\n",
      "episode: 76/433, score: 210.0, e: 0\n",
      "episode: 77/433, score: 210.0, e: 0\n",
      "episode: 78/433, score: 210.0, e: 0\n",
      "episode: 79/433, score: 210.0, e: 0\n",
      "episode: 80/433, score: 210.0, e: 0\n",
      "episode: 81/433, score: 210.0, e: 0\n",
      "episode: 82/433, score: 210.0, e: 0\n",
      "episode: 83/433, score: 210.0, e: 0\n",
      "episode: 84/433, score: 210.0, e: 0\n",
      "episode: 85/433, score: 210.0, e: 0\n",
      "episode: 86/433, score: 210.0, e: 0\n",
      "episode: 87/433, score: 210.0, e: 0\n",
      "episode: 88/433, score: 207.0, e: 0\n",
      "episode: 89/433, score: 210.0, e: 0\n",
      "episode: 90/433, score: 210.0, e: 0\n",
      "episode: 91/433, score: 210.0, e: 0\n",
      "episode: 92/433, score: 210.0, e: 0\n",
      "episode: 93/433, score: 210.0, e: 0\n",
      "episode: 94/433, score: 210.0, e: 0\n",
      "episode: 95/433, score: 210.0, e: 0\n",
      "episode: 96/433, score: 210.0, e: 0\n",
      "episode: 97/433, score: 210.0, e: 0\n",
      "episode: 98/433, score: 210.0, e: 0\n",
      "episode: 99/433, score: 210.0, e: 0\n",
      "episode: 100/433, score: 210.0, e: 0\n",
      "episode: 101/433, score: 210.0, e: 0\n",
      "episode: 102/433, score: 210.0, e: 0\n",
      "episode: 103/433, score: 210.0, e: 0\n",
      "episode: 104/433, score: 210.0, e: 0\n",
      "episode: 105/433, score: 210.0, e: 0\n",
      "episode: 106/433, score: 210.0, e: 0\n",
      "episode: 107/433, score: 210.0, e: 0\n",
      "episode: 108/433, score: 210.0, e: 0\n",
      "episode: 109/433, score: 210.0, e: 0\n",
      "episode: 110/433, score: 210.0, e: 0\n",
      "episode: 111/433, score: 210.0, e: 0\n",
      "episode: 112/433, score: 210.0, e: 0\n",
      "episode: 113/433, score: 210.0, e: 0\n",
      "episode: 114/433, score: 210.0, e: 0\n",
      "episode: 115/433, score: 210.0, e: 0\n",
      "episode: 116/433, score: 210.0, e: 0\n",
      "episode: 117/433, score: 210.0, e: 0\n",
      "episode: 118/433, score: 210.0, e: 0\n",
      "episode: 119/433, score: 210.0, e: 0\n",
      "episode: 120/433, score: 210.0, e: 0\n",
      "episode: 121/433, score: 210.0, e: 0\n",
      "episode: 122/433, score: 210.0, e: 0\n",
      "episode: 123/433, score: 210.0, e: 0\n",
      "episode: 124/433, score: 210.0, e: 0\n",
      "episode: 125/433, score: 207.0, e: 0\n",
      "episode: 126/433, score: 210.0, e: 0\n",
      "episode: 127/433, score: 210.0, e: 0\n",
      "episode: 128/433, score: 210.0, e: 0\n",
      "episode: 129/433, score: 210.0, e: 0\n",
      "episode: 130/433, score: 210.0, e: 0\n",
      "episode: 131/433, score: 210.0, e: 0\n",
      "episode: 132/433, score: 210.0, e: 0\n",
      "episode: 133/433, score: 210.0, e: 0\n",
      "episode: 134/433, score: 210.0, e: 0\n",
      "episode: 135/433, score: 210.0, e: 0\n",
      "episode: 136/433, score: 210.0, e: 0\n",
      "episode: 137/433, score: 210.0, e: 0\n",
      "episode: 138/433, score: 210.0, e: 0\n",
      "episode: 139/433, score: 210.0, e: 0\n",
      "episode: 140/433, score: 210.0, e: 0\n",
      "episode: 141/433, score: 210.0, e: 0\n",
      "episode: 142/433, score: 210.0, e: 0\n",
      "episode: 143/433, score: 210.0, e: 0\n",
      "episode: 144/433, score: 210.0, e: 0\n",
      "episode: 145/433, score: 210.0, e: 0\n",
      "episode: 146/433, score: 210.0, e: 0\n",
      "episode: 147/433, score: 210.0, e: 0\n",
      "episode: 148/433, score: 207.0, e: 0\n",
      "episode: 149/433, score: 210.0, e: 0\n",
      "episode: 150/433, score: 210.0, e: 0\n",
      "episode: 151/433, score: 210.0, e: 0\n",
      "episode: 152/433, score: 210.0, e: 0\n",
      "episode: 153/433, score: 210.0, e: 0\n",
      "episode: 154/433, score: 210.0, e: 0\n",
      "episode: 155/433, score: 210.0, e: 0\n",
      "episode: 156/433, score: 210.0, e: 0\n",
      "episode: 157/433, score: 210.0, e: 0\n",
      "episode: 158/433, score: 210.0, e: 0\n",
      "episode: 159/433, score: 210.0, e: 0\n",
      "episode: 160/433, score: 210.0, e: 0\n",
      "episode: 161/433, score: 210.0, e: 0\n",
      "episode: 162/433, score: 210.0, e: 0\n",
      "episode: 163/433, score: 210.0, e: 0\n",
      "episode: 164/433, score: 210.0, e: 0\n",
      "episode: 165/433, score: 210.0, e: 0\n",
      "episode: 166/433, score: 210.0, e: 0\n",
      "episode: 167/433, score: 210.0, e: 0\n",
      "episode: 168/433, score: 210.0, e: 0\n",
      "episode: 169/433, score: 210.0, e: 0\n",
      "episode: 170/433, score: 210.0, e: 0\n",
      "episode: 171/433, score: 210.0, e: 0\n",
      "episode: 172/433, score: 210.0, e: 0\n",
      "episode: 173/433, score: 210.0, e: 0\n",
      "episode: 174/433, score: 210.0, e: 0\n",
      "episode: 175/433, score: 210.0, e: 0\n",
      "episode: 176/433, score: 210.0, e: 0\n",
      "episode: 177/433, score: 210.0, e: 0\n",
      "episode: 178/433, score: 210.0, e: 0\n",
      "episode: 179/433, score: 210.0, e: 0\n",
      "episode: 180/433, score: 210.0, e: 0\n",
      "episode: 181/433, score: 210.0, e: 0\n",
      "episode: 182/433, score: 210.0, e: 0\n",
      "episode: 183/433, score: 210.0, e: 0\n",
      "episode: 184/433, score: 210.0, e: 0\n",
      "episode: 185/433, score: 210.0, e: 0\n",
      "episode: 186/433, score: 210.0, e: 0\n",
      "episode: 187/433, score: 210.0, e: 0\n",
      "episode: 188/433, score: 210.0, e: 0\n",
      "episode: 189/433, score: 210.0, e: 0\n",
      "episode: 190/433, score: 210.0, e: 0\n",
      "episode: 191/433, score: 210.0, e: 0\n",
      "episode: 192/433, score: 210.0, e: 0\n",
      "episode: 193/433, score: 210.0, e: 0\n",
      "episode: 194/433, score: 210.0, e: 0\n",
      "episode: 195/433, score: 210.0, e: 0\n",
      "episode: 196/433, score: 210.0, e: 0\n",
      "episode: 197/433, score: 210.0, e: 0\n",
      "episode: 198/433, score: 210.0, e: 0\n",
      "episode: 199/433, score: 210.0, e: 0\n",
      "episode: 200/433, score: 210.0, e: 0\n",
      "episode: 201/433, score: 210.0, e: 0\n",
      "episode: 202/433, score: 210.0, e: 0\n",
      "episode: 203/433, score: 210.0, e: 0\n",
      "episode: 204/433, score: 210.0, e: 0\n",
      "episode: 205/433, score: 210.0, e: 0\n",
      "episode: 206/433, score: 210.0, e: 0\n",
      "episode: 207/433, score: 210.0, e: 0\n",
      "episode: 208/433, score: 210.0, e: 0\n",
      "episode: 209/433, score: 210.0, e: 0\n",
      "episode: 210/433, score: 210.0, e: 0\n",
      "episode: 211/433, score: 210.0, e: 0\n",
      "episode: 212/433, score: 210.0, e: 0\n",
      "episode: 213/433, score: 210.0, e: 0\n",
      "episode: 214/433, score: 210.0, e: 0\n",
      "episode: 215/433, score: 210.0, e: 0\n",
      "episode: 216/433, score: 210.0, e: 0\n",
      "episode: 217/433, score: 210.0, e: 0\n",
      "episode: 218/433, score: 210.0, e: 0\n",
      "episode: 219/433, score: 210.0, e: 0\n",
      "episode: 220/433, score: 210.0, e: 0\n",
      "episode: 221/433, score: 210.0, e: 0\n",
      "episode: 222/433, score: 210.0, e: 0\n",
      "episode: 223/433, score: 210.0, e: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 224/433, score: 210.0, e: 0\n",
      "episode: 225/433, score: 210.0, e: 0\n",
      "episode: 226/433, score: 210.0, e: 0\n",
      "episode: 227/433, score: 210.0, e: 0\n",
      "episode: 228/433, score: 210.0, e: 0\n",
      "episode: 229/433, score: 210.0, e: 0\n",
      "episode: 230/433, score: 210.0, e: 0\n",
      "episode: 231/433, score: 210.0, e: 0\n",
      "episode: 232/433, score: 210.0, e: 0\n",
      "episode: 233/433, score: 210.0, e: 0\n",
      "episode: 234/433, score: 210.0, e: 0\n",
      "episode: 235/433, score: 210.0, e: 0\n",
      "episode: 236/433, score: 210.0, e: 0\n",
      "episode: 237/433, score: 210.0, e: 0\n",
      "episode: 238/433, score: 210.0, e: 0\n",
      "episode: 239/433, score: 210.0, e: 0\n",
      "episode: 240/433, score: 210.0, e: 0\n",
      "episode: 241/433, score: 210.0, e: 0\n",
      "episode: 242/433, score: 210.0, e: 0\n",
      "episode: 243/433, score: 210.0, e: 0\n",
      "episode: 244/433, score: 210.0, e: 0\n",
      "episode: 245/433, score: 210.0, e: 0\n",
      "episode: 246/433, score: 210.0, e: 0\n",
      "episode: 247/433, score: 210.0, e: 0\n",
      "episode: 248/433, score: 210.0, e: 0\n",
      "episode: 249/433, score: 210.0, e: 0\n",
      "episode: 250/433, score: 210.0, e: 0\n",
      "episode: 251/433, score: 210.0, e: 0\n",
      "episode: 252/433, score: 210.0, e: 0\n",
      "episode: 253/433, score: 210.0, e: 0\n",
      "episode: 254/433, score: 210.0, e: 0\n",
      "episode: 255/433, score: 210.0, e: 0\n",
      "episode: 256/433, score: 210.0, e: 0\n",
      "episode: 257/433, score: 210.0, e: 0\n",
      "episode: 258/433, score: 210.0, e: 0\n",
      "episode: 259/433, score: 210.0, e: 0\n",
      "episode: 260/433, score: 210.0, e: 0\n",
      "episode: 261/433, score: 210.0, e: 0\n",
      "episode: 262/433, score: 210.0, e: 0\n",
      "episode: 263/433, score: 210.0, e: 0\n",
      "episode: 264/433, score: 210.0, e: 0\n",
      "episode: 265/433, score: 210.0, e: 0\n",
      "episode: 266/433, score: 210.0, e: 0\n",
      "episode: 267/433, score: 210.0, e: 0\n",
      "episode: 268/433, score: 210.0, e: 0\n",
      "episode: 269/433, score: 210.0, e: 0\n",
      "episode: 270/433, score: 210.0, e: 0\n",
      "episode: 271/433, score: 210.0, e: 0\n",
      "episode: 272/433, score: 210.0, e: 0\n",
      "episode: 273/433, score: 210.0, e: 0\n",
      "episode: 274/433, score: 210.0, e: 0\n",
      "episode: 275/433, score: 210.0, e: 0\n",
      "episode: 276/433, score: 210.0, e: 0\n",
      "episode: 277/433, score: 210.0, e: 0\n",
      "episode: 278/433, score: 210.0, e: 0\n",
      "episode: 279/433, score: 210.0, e: 0\n",
      "episode: 280/433, score: 210.0, e: 0\n",
      "episode: 281/433, score: 210.0, e: 0\n",
      "episode: 282/433, score: 210.0, e: 0\n",
      "episode: 283/433, score: 210.0, e: 0\n",
      "episode: 284/433, score: 210.0, e: 0\n",
      "episode: 285/433, score: 210.0, e: 0\n",
      "episode: 286/433, score: 210.0, e: 0\n",
      "episode: 287/433, score: 210.0, e: 0\n",
      "episode: 288/433, score: 210.0, e: 0\n",
      "episode: 289/433, score: 210.0, e: 0\n",
      "episode: 290/433, score: 210.0, e: 0\n",
      "episode: 291/433, score: 210.0, e: 0\n",
      "episode: 292/433, score: 210.0, e: 0\n",
      "episode: 293/433, score: 210.0, e: 0\n",
      "episode: 294/433, score: 210.0, e: 0\n",
      "episode: 295/433, score: 210.0, e: 0\n",
      "episode: 296/433, score: 210.0, e: 0\n",
      "episode: 297/433, score: 210.0, e: 0\n",
      "episode: 298/433, score: 210.0, e: 0\n",
      "episode: 299/433, score: 210.0, e: 0\n",
      "episode: 300/433, score: 210.0, e: 0\n",
      "episode: 301/433, score: 210.0, e: 0\n",
      "episode: 302/433, score: 210.0, e: 0\n",
      "episode: 303/433, score: 210.0, e: 0\n",
      "episode: 304/433, score: 210.0, e: 0\n",
      "episode: 305/433, score: 210.0, e: 0\n",
      "episode: 306/433, score: 210.0, e: 0\n",
      "episode: 307/433, score: 210.0, e: 0\n",
      "episode: 308/433, score: 210.0, e: 0\n",
      "episode: 309/433, score: 210.0, e: 0\n",
      "episode: 310/433, score: 210.0, e: 0\n",
      "episode: 311/433, score: 210.0, e: 0\n",
      "episode: 312/433, score: 210.0, e: 0\n",
      "episode: 313/433, score: 210.0, e: 0\n",
      "episode: 314/433, score: 210.0, e: 0\n",
      "episode: 315/433, score: 210.0, e: 0\n",
      "episode: 316/433, score: 210.0, e: 0\n",
      "episode: 317/433, score: 210.0, e: 0\n",
      "episode: 318/433, score: 210.0, e: 0\n",
      "episode: 319/433, score: 210.0, e: 0\n",
      "episode: 320/433, score: 210.0, e: 0\n",
      "episode: 321/433, score: 210.0, e: 0\n",
      "episode: 322/433, score: 210.0, e: 0\n",
      "episode: 323/433, score: 210.0, e: 0\n",
      "episode: 324/433, score: 210.0, e: 0\n",
      "episode: 325/433, score: 210.0, e: 0\n",
      "episode: 326/433, score: 210.0, e: 0\n",
      "episode: 327/433, score: 210.0, e: 0\n",
      "episode: 328/433, score: 210.0, e: 0\n",
      "episode: 329/433, score: 210.0, e: 0\n",
      "episode: 330/433, score: 210.0, e: 0\n",
      "episode: 331/433, score: 210.0, e: 0\n",
      "episode: 332/433, score: 210.0, e: 0\n",
      "episode: 333/433, score: 210.0, e: 0\n",
      "episode: 334/433, score: 210.0, e: 0\n",
      "episode: 335/433, score: 210.0, e: 0\n",
      "episode: 336/433, score: 210.0, e: 0\n",
      "episode: 337/433, score: 210.0, e: 0\n",
      "episode: 338/433, score: 210.0, e: 0\n",
      "episode: 339/433, score: 210.0, e: 0\n",
      "episode: 340/433, score: 210.0, e: 0\n",
      "episode: 341/433, score: 210.0, e: 0\n",
      "episode: 342/433, score: 210.0, e: 0\n",
      "episode: 343/433, score: 210.0, e: 0\n",
      "episode: 344/433, score: 210.0, e: 0\n",
      "episode: 345/433, score: 210.0, e: 0\n",
      "episode: 346/433, score: 210.0, e: 0\n",
      "episode: 347/433, score: 210.0, e: 0\n",
      "episode: 348/433, score: 210.0, e: 0\n",
      "episode: 349/433, score: 210.0, e: 0\n",
      "episode: 350/433, score: 210.0, e: 0\n",
      "episode: 351/433, score: 210.0, e: 0\n",
      "episode: 352/433, score: 210.0, e: 0\n",
      "episode: 353/433, score: 210.0, e: 0\n",
      "episode: 354/433, score: 210.0, e: 0\n",
      "episode: 355/433, score: 210.0, e: 0\n",
      "episode: 356/433, score: 210.0, e: 0\n",
      "episode: 357/433, score: 210.0, e: 0\n",
      "episode: 358/433, score: 210.0, e: 0\n",
      "episode: 359/433, score: 210.0, e: 0\n",
      "episode: 360/433, score: 210.0, e: 0\n",
      "episode: 361/433, score: 210.0, e: 0\n",
      "episode: 362/433, score: 210.0, e: 0\n",
      "episode: 363/433, score: 210.0, e: 0\n",
      "episode: 364/433, score: 210.0, e: 0\n",
      "episode: 365/433, score: 193.0, e: 0\n",
      "episode: 366/433, score: 210.0, e: 0\n",
      "episode: 367/433, score: 210.0, e: 0\n",
      "episode: 368/433, score: 210.0, e: 0\n",
      "episode: 369/433, score: 210.0, e: 0\n",
      "episode: 370/433, score: 210.0, e: 0\n",
      "episode: 371/433, score: 210.0, e: 0\n",
      "episode: 372/433, score: 210.0, e: 0\n",
      "episode: 373/433, score: 210.0, e: 0\n",
      "episode: 374/433, score: 210.0, e: 0\n",
      "episode: 375/433, score: 210.0, e: 0\n",
      "episode: 376/433, score: 210.0, e: 0\n",
      "episode: 377/433, score: 210.0, e: 0\n",
      "episode: 378/433, score: 210.0, e: 0\n",
      "episode: 379/433, score: 210.0, e: 0\n",
      "episode: 380/433, score: 210.0, e: 0\n",
      "episode: 381/433, score: 210.0, e: 0\n",
      "episode: 382/433, score: 210.0, e: 0\n",
      "episode: 383/433, score: 210.0, e: 0\n",
      "episode: 384/433, score: 210.0, e: 0\n",
      "episode: 385/433, score: 210.0, e: 0\n",
      "episode: 386/433, score: 210.0, e: 0\n",
      "episode: 387/433, score: 210.0, e: 0\n",
      "episode: 388/433, score: 210.0, e: 0\n",
      "episode: 389/433, score: 210.0, e: 0\n",
      "episode: 390/433, score: 210.0, e: 0\n",
      "episode: 391/433, score: 210.0, e: 0\n",
      "episode: 392/433, score: 210.0, e: 0\n",
      "episode: 393/433, score: 210.0, e: 0\n",
      "episode: 394/433, score: 210.0, e: 0\n",
      "episode: 395/433, score: 210.0, e: 0\n",
      "episode: 396/433, score: 210.0, e: 0\n",
      "episode: 397/433, score: 210.0, e: 0\n",
      "episode: 398/433, score: 210.0, e: 0\n",
      "episode: 399/433, score: 210.0, e: 0\n",
      "episode: 400/433, score: 210.0, e: 0\n",
      "episode: 401/433, score: 210.0, e: 0\n",
      "episode: 402/433, score: 210.0, e: 0\n",
      "episode: 403/433, score: 210.0, e: 0\n",
      "episode: 404/433, score: 210.0, e: 0\n",
      "episode: 405/433, score: 210.0, e: 0\n",
      "episode: 406/433, score: 210.0, e: 0\n",
      "episode: 407/433, score: 210.0, e: 0\n",
      "episode: 408/433, score: 210.0, e: 0\n",
      "episode: 409/433, score: 210.0, e: 0\n",
      "episode: 410/433, score: 210.0, e: 0\n",
      "episode: 411/433, score: 210.0, e: 0\n",
      "episode: 412/433, score: 210.0, e: 0\n",
      "episode: 413/433, score: 210.0, e: 0\n",
      "episode: 414/433, score: 210.0, e: 0\n",
      "episode: 415/433, score: 210.0, e: 0\n",
      "episode: 416/433, score: 210.0, e: 0\n",
      "episode: 417/433, score: 210.0, e: 0\n",
      "episode: 418/433, score: 210.0, e: 0\n",
      "episode: 419/433, score: 210.0, e: 0\n",
      "episode: 420/433, score: 210.0, e: 0\n",
      "episode: 421/433, score: 210.0, e: 0\n",
      "episode: 422/433, score: 210.0, e: 0\n",
      "episode: 423/433, score: 210.0, e: 0\n",
      "episode: 424/433, score: 210.0, e: 0\n",
      "episode: 425/433, score: 210.0, e: 0\n",
      "episode: 426/433, score: 210.0, e: 0\n",
      "episode: 427/433, score: 210.0, e: 0\n",
      "episode: 428/433, score: 210.0, e: 0\n",
      "episode: 429/433, score: 210.0, e: 0\n",
      "episode: 430/433, score: 210.0, e: 0\n",
      "episode: 431/433, score: 210.0, e: 0\n",
      "episode: 432/433, score: 210.0, e: 0\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "print('Training complete. Testing started...')\n",
    "#TEST Time\n",
    "#   In this section we ALWAYS use exploit don't train any more\n",
    "for e_test in range(TEST_Episodes):\n",
    "    state = envCartPole.reset()\n",
    "    state = np.reshape(state, [1, nS])\n",
    "    tot_rewards = 0\n",
    "    for t_test in range(210):\n",
    "        action = dqn.test_action(state)\n",
    "        nstate, reward, done, _ = envCartPole.step(action)\n",
    "        nstate = np.reshape( nstate, [1, nS])\n",
    "        tot_rewards += reward\n",
    "        #DON'T STORE ANYTHING DURING TESTING\n",
    "        state = nstate\n",
    "        #done: CartPole fell. \n",
    "        #t_test == 209: CartPole stayed upright\n",
    "        if done or t_test == 209: \n",
    "            rewards.append(tot_rewards)\n",
    "            epsilons.append(0) #We are doing full exploit\n",
    "            print(\"episode: {}/{}, score: {}, e: {}\"\n",
    "                  .format(e_test, TEST_Episodes, tot_rewards, 0))\n",
    "            break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**  \n",
    "Here is a graph of the results. If everything was done correctly you should see the rewards over the red line.  \n",
    "\n",
    "Black: This is the 100 episode rolling average  \n",
    "Red: This is the \"solved\" line at 195  \n",
    "Blue: This is the reward for each episode  \n",
    "Green: This is the value of epsilon scaled by 200  \n",
    "Yellow: This is where the tests started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x230c1b35a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting\n",
    "rolling_average = np.convolve(rewards, np.ones(100)/100)\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.plot(rolling_average, color='black')\n",
    "plt.axhline(y=195, color='r', linestyle='-') #Solved Line\n",
    "#Scale Epsilon (0.001 - 1.0) to match reward (0 - 200) range\n",
    "eps_graph = [200*x for x in epsilons]\n",
    "plt.plot(eps_graph, color='g', linestyle='-')\n",
    "#Plot the line where TESTING begins\n",
    "plt.axvline(x=TRAIN_END, color='y', linestyle='-')\n",
    "plt.xlim( (0,EPISODES) )\n",
    "plt.ylim( (0,220) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Changes**  \n",
    "These are all the same changes as the DQN notebook with the exception of the update weights parameter.  \n",
    "*hyper parameters*: You can alter alpha, gamma, batch size, and episode length to see what differences the algorithm returns.  \n",
    "*Training End*: You can also change the line where I only check the last 5 runs before switching to testing mode (if len(rewards) > 5 and np.average(rewards[-5:]) > 195:) as that doesn't prove it was solved. The reason I did this was because I wanted to limit the amount of runs I made.  \n",
    "*Update Weights*: I call 'dqn.update_target_from_model()' after every episode. You can adjust this to run at different times. I have done per step (no matter how long the episode ran) and I have seen it done every X episodes. Feel free to try different things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**  \n",
    "This is a Double Deep Q-Network implementation. There are some changes you can make here and there but it follows the paper as close as I could. If you want to dive deeper you can see that the paper has graphs that dive deeper into the inner workings of the neural network.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference**  \n",
    "Van Hasselt, H., Guez, A., & Silver, D. (2016, February). Deep Reinforcement Learning with Double Q-Learning. In AAAI (Vol. 2, p. 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Conclusion  \n",
    "This completes the set of notebooks that cover the original Q-Learner and continues through recent updates. Once you have worked in this area for a while you can see how powerful that first update statement really was. With just some slight tweaks, these algorithms were able to achieve higher scores than even advanced Atari players.  \n",
    "\n",
    "I hope these notebooks have peaked your interest enough to continue your reinforcement journey."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
