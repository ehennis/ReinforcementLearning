{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Q-Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Double Q-Learning was created by Hado van Hasselt in 2010. The idea is that because a normal q-learner uses *max* that it overestimates the action values. To help solve this Hasselt introduces a second Q table that we will use to figure out the max value action estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the abstract from the paper:  \n",
    "In some stochastic environments the well-known reinforcement learning algorithm\n",
    "Q-learning performs very poorly. This poor performance is caused by large\n",
    "overestimations of action values. These overestimations result from a positive\n",
    "bias that is introduced because Q-learning uses the maximum action value as an\n",
    "approximation for the maximum expected action value. We introduce an alternative\n",
    "way to approximate the maximum expected value for any set of random\n",
    "variables. The obtained double estimator method is shown to sometimes underestimate\n",
    "rather than overestimate the maximum expected value. We apply the\n",
    "double estimator to Q-learning to construct Double Q-learning, a new off-policy\n",
    "reinforcement learning algorithm. We show the new algorithm converges to the\n",
    "optimal policy and that it performs well in some settings in which Q-learning performs\n",
    "poorly due to its overestimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Equations**   \n",
    "Remember, this is the Q-learning update formula  \n",
    "$$Q'(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha * \\big( r_t + \\gamma * max_a Q(s_{t+1},a) - Q(s_t,a_t) \\big) $$  \n",
    "\n",
    "When using a Double Q-learner you will have 2 update equations that you randomly select which one you update. \n",
    "Choose UPDATE(A) or UPDATE(B) randomly  \n",
    "UPDATE(A)\n",
    "$$a^* = arg \\, max_a Q^A(s',a)$$\n",
    "$$Q^A(s,a) \\leftarrow Q^A(s,a) + \\alpha(s,a) * \\big( r + \\gamma * Q^B(s',a^*) - Q^A(s,a) \\big) $$\n",
    "UPDATE(B)\n",
    "$$a^* = arg \\, max_a Q^B(s',a)$$\n",
    "$$Q^B(s,a) \\leftarrow Q^B(s,a) + \\alpha(s,a) * \\big( r + \\gamma * Q^A(s',a^*) - Q^B(s,a) \\big) $$  \n",
    "\n",
    "Now, to put this in simplier terms, if you are doing UPDATE(A) you will do the equation like normal BUT when you find the future rewards ($Q^B(s',a^*)$) you are finding the **MAX** value from the $Q^A$ table and taking that index and getting the actual value from $Q^B$.  \n",
    "\n",
    "In an attempt to cement this in your brains here is an example:  \n",
    "$Q^A$ has this entry in state X: [ 0.1, 0.2, 0.12, 0.005]  \n",
    "$Q^B$ has this entry in state X: [ 0.9, 0.5, 0.30, 0.020]  \n",
    "\n",
    "What is the expected future value if you are doing UPDATE(A)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect.\n",
      "  First, you need to find the index of MAX(Q^A)\n",
      "  Then, you find the expected value at that index in Q^B\n"
     ]
    }
   ],
   "source": [
    "from doubleq import DoubleQQuestion1\n",
    "DoubleQQuestion1(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the double Q algorithm. Maybe have them create the calcuations??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*References*  \n",
    "Hasselt, H. V. (2010). Double Q-learning. Advances in Neural Information Processing Systems 23,2613-2621. Retrieved from http://papers.nips.cc/paper/3964-double-q-learning.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
