{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Q-Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Double Q-Learning was created by Hado van Hasselt in 2010. The idea is that because a normal q-learner uses *max* that it overestimates the action values. To help solve this Hasselt introduces a second Q table that we will use to figure out the max value action estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the abstract from the paper:  \n",
    "In some stochastic environments the well-known reinforcement learning algorithm\n",
    "Q-learning performs very poorly. This poor performance is caused by large\n",
    "overestimations of action values. These overestimations result from a positive\n",
    "bias that is introduced because Q-learning uses the maximum action value as an\n",
    "approximation for the maximum expected action value. We introduce an alternative\n",
    "way to approximate the maximum expected value for any set of random\n",
    "variables. The obtained double estimator method is shown to sometimes underestimate\n",
    "rather than overestimate the maximum expected value. We apply the\n",
    "double estimator to Q-learning to construct Double Q-learning, a new off-policy\n",
    "reinforcement learning algorithm. We show the new algorithm converges to the\n",
    "optimal policy and that it performs well in some settings in which Q-learning performs\n",
    "poorly due to its overestimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Equations**   \n",
    "Remember, this is the Q-learning update formula  \n",
    "$$Q'(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha * \\big( r_t + \\gamma * max_a Q(s_{t+1},a) - Q(s_t,a_t) \\big) $$  \n",
    "\n",
    "When using a Double Q-learner you will have 2 update equations that you randomly select which one you update. \n",
    "Choose UPDATE(A) or UPDATE(B) randomly  \n",
    "UPDATE(A)\n",
    "$$a^* = arg \\, max_a Q^A(s',a)$$\n",
    "$$Q^A(s,a) \\leftarrow Q^A(s,a) + \\alpha(s,a) * \\big( r + \\gamma * Q^B(s',a^*) - Q^A(s,a) \\big) $$\n",
    "UPDATE(B)\n",
    "$$a^* = arg \\, max_a Q^B(s',a)$$\n",
    "$$Q^B(s,a) \\leftarrow Q^B(s,a) + \\alpha(s,a) * \\big( r + \\gamma * Q^A(s',a^*) - Q^B(s,a) \\big) $$  \n",
    "\n",
    "Now, to put this in simplier terms, if you are doing UPDATE(A) you will do the equation like normal BUT when you find the future rewards ($Q^B(s',a^*)$) you are finding the **MAX** value from the $Q^A$ table and taking that index and getting the actual value from $Q^B$.  \n",
    "\n",
    "In an attempt to cement this in your brains here is an example:  \n",
    "$Q^A$ has this entry in state X: [ 0.1, 0.2, 0.12, 0.005]  \n",
    "$Q^B$ has this entry in state X: [ 0.9, 0.5, 0.30, 0.020]  \n",
    "\n",
    "What is the expected future value if you are doing UPDATE(A)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect.\n",
      "  First, you need to find the index of MAX(Q^A)\n",
      "  Then, you find the expected value at that index in Q^B\n"
     ]
    }
   ],
   "source": [
    "from doubleq import DoubleQQuestion1\n",
    "DoubleQQuestion1(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to do one more question before we get into the implementation of a full Double Q-Leaner.  \n",
    "\n",
    "Code up both UpdateA and UpdateB and then run the test below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def UpdateA(QA, QB, state, action, nstate, alpha=1, reward=1, gamma=1, done=False):\n",
    "    #Find a* (Index of the max value in s') in QA\n",
    "    a_star_idx = 0 #TODO\n",
    "    #Find QB[s',a*]\n",
    "    future_value = 0 #TODO\n",
    "    QA[state][action] += alpha * (reward + gamma * future_value * (not done) - QA[state][action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def UpdateB(QA, QB, state, action, nstate, alpha=1, reward=1, gamma=1, done=False):\n",
    "    #Find a* (Index of the max value in s') in QA\n",
    "    a_star_idx = 0 #TODO\n",
    "    #Find QA[s',a*]\n",
    "    future_value = 0 #TODO\n",
    "    QB[state][action] += alpha * (reward + gamma * future_value * (not done) - QB[state][action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail. Try again.\n",
      "Fail. Try again.\n"
     ]
    }
   ],
   "source": [
    "nS = 10 #10 States\n",
    "nA = 4 #4 Actions\n",
    "QA = np.zeros((nS,nA), dtype=np.float)\n",
    "QB = np.zeros((nS,nA), dtype=np.float)\n",
    "\n",
    "#Set the maximum expected value for each of the 4 actions for state 1\n",
    "QA[1] = (1,2,3,4)\n",
    "QB[1] = (8,7,6,5)\n",
    "\n",
    "QA[2] = (5,6,2,1)\n",
    "QB[2] = (7,5,6,4)\n",
    "\n",
    "#Starting at state 1, take action 3, and end up in state 2\n",
    "UpdateA(QA,QB, 1, 3, 2)\n",
    "if QA[1,3] == 6.0:\n",
    "    print('Success')\n",
    "else:\n",
    "    print('Fail. Try again.')\n",
    "#Repeat with QB being randomly selected\n",
    "UpdateB(QA,QB, 1, 3, 2)\n",
    "if QB[1,3] == 6.0:\n",
    "    print('Success')\n",
    "else:\n",
    "    print('Fail. Try again.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were having troubles here is how I would explain what was going on. I am going to use UpdateA as an example as it should be easy enough for you to switch $Q^A$ and $Q^B$.  \n",
    "\n",
    "To find $$a^* = arg \\, max_a Q^A(s',a)$$ you need to find the index of the max value in the next state (nstate in code) of  $Q^A$. Numpy has np.argmax for just this reason.  \n",
    "CODE: a_star_idx = np.argmax(QB[nstate])\n",
    "\n",
    "To find $$Q^B(s',a^*)$$ you would take the index from the previous answer and put it against the next state (nstate) of $Q^B$.  \n",
    "CODE: future_value = QA[nstate,a_star_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*References*  \n",
    "Hasselt, H. V. (2010). Double Q-learning. Advances in Neural Information Processing Systems 23,2613-2621. Retrieved from http://papers.nips.cc/paper/3964-double-q-learning.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
