{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**  \n",
    "Q-learning is a reinforcement learning technique. The goal is to learn the optimal policy. It is model free meaning that it doesn't need the entire environment to run. Q-learning involves an agent and an environment. The environment is a set of states S, and a set of actions A that are allowed in each state. The agent will reside in a state and perform an action. The environment will process that action and return back the new state (could be the same) and the reward for that action. The goal of the agent is to find the optimal policy with the largest reward. Just like with the MDPs in the previous notebook, the agent calculates the maximum future reward.  \n",
    "\n",
    "**Learning Rate**: The learning rate, or \\alpha, is a value between 0 and 1. It determines how aggressively you update the table value. With a learning rate near 0 you will not update your table value much. With the learning rate near 1 you are almost replacing the current value with the new calculated value. Basically, you ignore most of what you had learned for what you picked up this time. Typically, this is around 0.1.\n",
    "\n",
    "**Explore/Exploit**: Imagine a baby that is trying to learn how to accomplish a task. At first they are just a tornado of arms and legs flailing about. But, given enough time they figure out what they need to accomplish. The flailing about is the exploring part and the part where they know what to do is exploiting their knowledge.  \n",
    "  \n",
    "A major part of the q-learning agent is whether to explore the environment or exploit the environment. Initially, everything is exploring as the agent hasn't learned anything about the environment (in most implementations this is done by setting the initial values of all the actions within the state to a random number). When the agent starts training you will need to determine how much of the time do you take random actions and how much of the time do you take the optimal action. If you explore for too long you won't learn the optimal policy because all of your actions will be random and if you exploit too much you have the possibility of never finding the optimal solution. This process is called *epsilon-greedy* where epsilon is the percent of time the agent chooses to explore. In most problems the ideal explore rate is 10%. There are some algorithms that have this value decay over time to take advantage of your training.  \n",
    "\n",
    "**Discount Factor**: As stated in the MDP section, The discount factor is between 0 and 1. This determines how much you want to give the future path credit. You need to balance out your immediate rewards versus your future rewards. The higher the discount factor the further into the future path you want to include in this state/action pair. I will record the same math here as before to help this make sense. If your discount factor is 0.8 and after 5 steps you get a reward of 4 the present value of that reward is $0.8^4 * 5$ or ~2. If you change the discount factor or 0.9 that value becomes ~3.2. 0.1 turns into 0.0005.\n",
    "  \n",
    "**Q Table**: In a standard Q-learning algorithm the agent holds a q table that it uses to determine the ideal action for each state. This table is S x A in size. For each state we store the reward for each action. Typically, this is done as a 2 dimensional array but you can use other data structures. Also, in deep learning the q table is a neural network.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm**  \n",
    "$$Q'(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha * \\big( r_t + \\gamma * max_a Q(s_{t+1},a) - Q(s_t,a_t) \\big) $$  \n",
    "  \n",
    "$\\alpha$: This is the learning rate. As listed above, this determines how much you change your Q value.  \n",
    "$Q(s_t,a_t)$: This is the old Q value from the table.  \n",
    "$r_t$: Is the reward you are receiving for taking action $a_t$ in state $s_t$  \n",
    "$\\gamma$: This is the discount factor similar to the MDP   \n",
    "$max_aQ(s_{t+1},a)$: This is the maximum future reward from state $s_{t+1}$.  \n",
    "$Q(s_t,a_t)$: This is the old Q value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Question 1**  \n",
    "A simple question to make sure you understand where all the pieces fit together.  \n",
    "First, the hyper parameters.  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Your learning rate ($\\alpha$) is 0.1  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Your discount rate ($\\gamma$\\) is 0.8.  \n",
    "You are in state S1 and taking action A1 to state S2 and getting the reward of 5.  \n",
    "What is the new Q[S1,A1] value assuming this following Q table?  \n",
    "\n",
    "| |A1|A2|A3|  \n",
    "|----------|----------|----------|---------|  \n",
    "|S1|0.1|0.2|0.3|\n",
    "|S2|0.4|0.5|0.6|  \n",
    "|S3|0.7|0.8|0.9|  \n",
    "|S4|0.11|0.12|0.13|  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct\n",
      "  The old value is 0.1\n",
      "  The learning rate (alpha) is 0.1\n",
      "  The reward is 5\n",
      "  The discount factor is 0.8\n",
      "  The max action from state S2 is A3 with a value of 0.6\n",
      "  0.1 + 0.1 * (5 + 0.8 * 0.6 - 0.1)\n",
      "  0.1 + 0.1 * (5 + 4.8 - 0.1)\n",
      "  0.1 + 0.1 * (5.38)\n",
      "  0.1 + 0.538\n",
      "  0.638\n"
     ]
    }
   ],
   "source": [
    "from qlearning import QLQuestion1 #Import solution file\n",
    "QLQuestion1(0.1) #Pass in an integer value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing I wanted to point out was finding $max_aQ(s_{t+1},a)$ in the previous question as it was hard for me to figure out until I did a few examples. In our problem we are looking at $max_a Q(S2,a)$. When we look at state S2 we see that we have 3 values, 0.4, 0.5, and 0.6. Clearly, the largest is under action A3. So, the $max_a$ in this case is A3. Now, we don't care which column it is from or care what action is taken. We just need the max expected value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discrete/Continuous Environments**\n",
    "\n",
    "A discrete environment is when you have a certain number of states. This can be anything from 1 up to a managable amount. If you have to have a supercomputer in order to store your Q table it isn't discrete. A continuous environment is more like the real world. You wouldn't be able to list the states in a simple action like walking across the room let alone a complex simulation.  \n",
    "\n",
    "If you want to still use a q-learner in a continuous state you have to use something called *discretization*. This is where you group the continuous states into discrete ranges. If you have seen a histogram you have seen discretization. When you break up the data into bins you are essentially making all the data fit into a discrete number of bars.  \n",
    "\n",
    "The simplest way to discretize a continuous space is to split the states into buckets. If you environment has 1 million states you could break them up into states of 10,000.  \n",
    "\n",
    "If you have a more complex environment you could break them into buckets based on how similar their task is to complete. For example, in soccer, if you are on the left side of the field you need to go to the right to get to the goal. If you are on the right you need to go left to get to the goal. You could split that into 2 states. Now, granted, that is terrible and would never work but you get the idea.  \n",
    "\n",
    "To get beyond discretization in continuous spaces and foreshadow future notebooks, you can use function approximation. Function approximation is something that is actually named correctly. You are asking for a function that will approximate the target function. In this world, Neural Networks are an example. We will see them when we get into deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning in a Discrete Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section I am going to have you create a Q-learner and have it go against an OpenAI environment called [FrozenLake](https://gym.openai.com/envs/FrozenLake-v0/). It is a 4x4 grid that has stochastic movements as well as holes that will end your game. But, for this question I am going to remove the stochastic movements at first. The reward for hitting the bottom right state is 1 while falling in a hole is 0. The goal is to start in the top left state and make it to the bottom right state without falling into the holes.  \n",
    "\n",
    "The environment looks like this:  \n",
    "SFFF  \n",
    "FHFH  \n",
    "FFFH  \n",
    "HFFG  \n",
    "  \n",
    "S : starting point, safe  \n",
    "F : frozen surface, safe  \n",
    "H : hole, fall to your doom  \n",
    "G : goal, where the frisbee is located  \n",
    "\n",
    "Actions:  \n",
    "0 : Left  \n",
    "1 : Down  \n",
    "2 : Right  \n",
    "3 : Up  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Cannot re-register id: FrozenLakeNotSlippery-v0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-9438fbf71b74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'FrozenLakeNotSlippery-v0'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mentry_point\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gym.envs.toy_text:FrozenLakeEnv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'map_name'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m'4x4'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'is_slippery'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'FrozenLakeNotSlippery-v0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nurl_\\anaconda3\\gym\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mregister\u001b[1;34m(id, **kwargs)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nurl_\\anaconda3\\gym\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mregister\u001b[1;34m(self, id, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot re-register id: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnvSpec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mError\u001b[0m: Cannot re-register id: FrozenLakeNotSlippery-v0"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import numpy as np\n",
    "\n",
    "#Create custom frozen lake without the stochastic movements\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    ")\n",
    "env = gym.make('FrozenLakeNotSlippery-v0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to look at the actions (action_space) and the size of the environment (observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n",
      "Discrete(16)\n"
     ]
    }
   ],
   "source": [
    "observations = env.reset()\n",
    "actions = env.action_space\n",
    "states = env.observation_space\n",
    "env.close()\n",
    "print(actions)\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are 4 actions (Left, Down, Right, Up) and there are 16 states (4x4 grid world).  \n",
    "\n",
    "Now, I want you to fill in the following code to implement a Q-learner. I have done most of it but I want you to create the formula. Remember, you need to recreate this:  \n",
    "\n",
    "$$Q'(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha * \\big( r_t + \\gamma * max_a Q(s_{t+1},a) - Q(s_t,a_t) \\big) $$  \n",
    "\n",
    "If you can't get it to work run the next section of code and it will print the equation that I used. You shouldn't have to alter any of the hyper parameters (alpha, gamma, epsilon, or epsilon decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is your policy\n",
      "D | R | D | L\n",
      "D | L | D | L\n",
      "R | D | D | L\n",
      "L | R | R | L\n",
      "\n",
      "Optimal Policy\n",
      "D | R | D | L\n",
      "D | L | D | L\n",
      "R | D | D | L\n",
      "L | R | R | L\n",
      "\n",
      "Q Table Values\n",
      "[[ 0.531441  0.59049   0.59049   0.531441]\n",
      " [ 0.531441  0.        0.6561    0.59049 ]\n",
      " [ 0.59049   0.729     0.59049   0.6561  ]\n",
      " [ 0.6561    0.        0.59049   0.59049 ]\n",
      " [ 0.59049   0.6561    0.        0.531441]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.81      0.        0.6561  ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.6561    0.        0.729     0.59049 ]\n",
      " [ 0.6561    0.81      0.81      0.      ]\n",
      " [ 0.729     0.9       0.        0.729   ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.81      0.9       0.729   ]\n",
      " [ 0.81      0.9       1.        0.81    ]\n",
      " [ 0.        0.        0.        0.      ]]\n"
     ]
    }
   ],
   "source": [
    "#Create a method that will be the Q-learner\n",
    "def convertDirection(q_val): #Convert the action int to a direction\n",
    "    if q_val == 0: return 'L'\n",
    "    if q_val == 1: return 'D'\n",
    "    if q_val == 2: return 'R'\n",
    "    if q_val == 3: return 'U'\n",
    "    return '?'\n",
    "\n",
    "\n",
    "def q_learning(env, alpha=0.1, gamma=0.9,  epsilon=0.99, epsilon_decay=0.99999):\n",
    "    nS = env.observation_space.n #Number of States\n",
    "    nA = env.action_space.n #Number of Actions\n",
    "    Q = np.zeros((nS,nA), dtype=np.float) #Initialize the Q table to all 0s\n",
    "    for e in range(10000): #Run 10k training runs\n",
    "        state = env.reset() #Part of OpenAI where you need to reset at the start of each run\n",
    "        total_reward = 0 #Set initial reward to 0\n",
    "        while True: #Loop until done == True\n",
    "            #IF random number is less than epsilon grab the random action else grab the argument max of Q[state]\n",
    "            action = env.action_space.sample() if np.random.random() < epsilon else np.argmax(Q[state])\n",
    "            nstate, reward, done, infor = env.step(action) #Send your action to OpenAI and get back the tuple\n",
    "            total_reward += reward #Increment your reward\n",
    "            #Q Function Update\n",
    "            #(not done) keeps the terminal state as 0\n",
    "            ######################################\n",
    "            ###                                ### \n",
    "            ### TODO: Implement this equations ###\n",
    "            ###                                ###\n",
    "            #This should take the form of:\n",
    "            #  Q[state][action] += [Add Learning Rate] * ([Add Q Update Code] * (not done) - Q[state][action])\n",
    "            Q[state][action] += 0\n",
    "            ######################################\n",
    "            state = nstate\n",
    "            if epsilon > 0.10: epsilon *= epsilon_decay #Make sure to keep random at 10%\n",
    "            if done:\n",
    "                break\n",
    "    pi = np.argmax(Q, axis=1) #Optimal Policy\n",
    "    return pi,Q\n",
    "\n",
    "pi, Q = q_learning(env) #Return the optimal policy and the Q table results\n",
    "\n",
    "print('Here is your policy')\n",
    "print('%s | %s | %s | %s' % (convertDirection(pi[0]),convertDirection(pi[1]),convertDirection(pi[2]),convertDirection(pi[3])))\n",
    "print('%s | %s | %s | %s' % (convertDirection(pi[4]),convertDirection(pi[5]),convertDirection(pi[6]),convertDirection(pi[7])))\n",
    "print('%s | %s | %s | %s' % (convertDirection(pi[8]),convertDirection(pi[9]),convertDirection(pi[10]),convertDirection(pi[11])))\n",
    "print('%s | %s | %s | %s' % (convertDirection(pi[12]),convertDirection(pi[13]),convertDirection(pi[14]),convertDirection(pi[15])))\n",
    "print('')\n",
    "print('Optimal Policy')\n",
    "print('D | R | D | L')\n",
    "print('D | L | D | L')\n",
    "print('R | D | D | L')\n",
    "print('L | R | R | L')\n",
    "print('')\n",
    "print('Q Table Values')\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is my solution\n",
      "  Q[state][action] += alpha * (reward + gamma * Q[nstate].max() * (not done) - Q[state][action])\n"
     ]
    }
   ],
   "source": [
    "#Run this code if you want the equation that I used\n",
    "from qlearning import QLSolution1 #Import solution file\n",
    "QLSolution1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to note:  \n",
    "* If your policy matched the optimal policy you have a successful Q-learner\n",
    "* When you look at the Q table you should see 0s for each of the holes\n",
    "* If I turn the slippery feature back on you would have a different Q table since you would get rewarded for moving away from the holes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Continuous Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain this section. Use discretization and then show its limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a problem that shows discretization. Possible have it generate numbers and show the bins"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
