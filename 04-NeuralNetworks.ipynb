{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Artificial Neural Networks (ANN) are a MASSIVE topic and I won't be ever able to cover all of it so I will do a trimmed down version that will cover enough to give you a background and make the Deep Q-Learning make sense in the next section.  \n",
    "\n",
    "A basic overview is that a neural network is based on the biological neural network in our brains. Our brains are built of very complex web of interconnected neurons. To get an idea of the scope, the human brain is estimated to contain $10^11$ neurons each connected to $10^4$ other neurons. The fastest times are known to be $10^{-3}$ seconds. Computers have speeds up to $10^{-10}$. But, Humans are able to make descisions extremely quick. For example, it takes $10^{-1}$ second to visually recognize your mother. This means that it can't take more than a few hundred steps. This has led many to speculate that the process is highly parallel. While a neuron in a biological neural network outputs a complex time series of spikes, an ANN returns a single constant value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Approximation**  \n",
    "As stated in the second notebook, one way that allows you to expand to larger problems is using function approximation. Function approximation is selecting a function to match up with your target. ANN can be used for a function approximator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptron**  \n",
    "A perceptron is a unit that will take in real-valued inputs (think the observations from CartPole) and calculates a linear combination and then outputs a value. In a simple example, imaging you have 3 inputs. Those 3 imputs will be mutliplied by 3 weights. The sum of those equations will then be returned as the output. It will look something like this:  \n",
    "$$\\sum_{i=1}^{3} X_i * W_i$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptron Rule**  \n",
    "The perceptron will alter the weights ($W_i$) until they are able to create a threshold that will separate the TRUE values and the FALSE values. Once we find the weights we will be able to create a line that will create half planes on a graph. Or, they will be linearly separable. It has been proven that this can be found in finite iterations (good news)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weight Update Walkthrough**  \n",
    "To update the equation you need to alter your value of $W$ until you have a satisfactory threshold. A quick note, I am going to ignore learning rate to make this easier. But, it normally would be part of the $\\Delta W_i$ equation.  \n",
    "\n",
    "To update the weights: $$W_i = W_i + \\Delta W_i$$\n",
    "To find $\\Delta W_i$: $$\\Delta W_i = (y - y')X_i$$  \n",
    "where $y$ is your target  \n",
    "$y'$ is your output and determined by this: $$y' = \\left( \\sum W_i X_i >= 0 \\right) $$\n",
    "$X_i$ is the input value.  \n",
    "\n",
    "Using binary:  \n",
    "  \n",
    "|y|y'| |y-y'|  \n",
    "|----------|----------|----------|----------|  \n",
    "|0|0| |0|  \n",
    "|0|1| |-1|  \n",
    "|1|0| |1|  \n",
    "|1|1| |0| \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example [AND]**  \n",
    "To start, I am going to cover a few VERY simple demo in order to understand how a neuron will work. In this demo, I am going to work through how the system would determine if your 2 variables were AND or not. Ex: $X_1$ = 1 and $X_2$ = 1. We do this by trying to create a graph that is linearly seperable. That is, when you can draw a line between the TRUE values and the FALSE values.  \n",
    "\n",
    "Using this formula: $\\sum X_i * W_i = \\theta$ where X is your variable and W is your weight and $\\theta$ is your threshold. Your X values are either 0 and 1 and your weights can be anything. In our example we will have $X_1 * W_1$ and $X_2 * W_2$ and some $\\theta$ value we will use as a threshold to determine if the results is TRUE or FALSE.  \n",
    "\n",
    "Our possible X values are {0,0}, {1,0}, {0,1}, and {1,1}  \n",
    "We need to set the weights ($W_1$ and $W_2$) and threshold ($\\theta$) so that we will only get TRUE for the pair that passes AND ({1,1}).\n",
    "For this problem we will set the threshold at 0.75\n",
    "\n",
    "Here are the equations:\n",
    "0 * W_1 + 0 * W_2 < 0.75 (FALSE)  \n",
    "1 * W_1 + 0 * W_2 < 0.75 (FALSE)  \n",
    "0 * W_1 + 1 * W_2 < 0.75 (FALSE)  \n",
    "0 * W_1 + 0 * W_2 > 0.75 (TRUE)  \n",
    "\n",
    "We can solve for this and find that $W_1$ = 0.5 and $W_2$ = 0.5 will satisfy all the equations. So, our weights for this would be 0.5 and 0.5.  \n",
    "\n",
    "Lets put this on a graph. We want to graph each of the points and then draw the line that will seperate the TRUE and FALSE sections of the graph.  \n",
    "\n",
    "First, find the values on each axis:  \n",
    "X1 Axis: We set $X_2$ to 0 and solve: $X_1$ * 0.5 + 0 * 0.5 = 0.75 => 1.5  \n",
    "X2 Axis: We set $X_1$ to 0 and solve: 0 * 0.5 + $X_2$ * 0.5 = 0.75 => 1.5  \n",
    "\n",
    "Second, connect the points with a line to show the half plane  \n",
    "\n",
    "This will show that anything above the half plane is TRUE and anything below it is false. If you plot {0,0} or {1,0} or {1,0} you can clearly see that they are in the green side where {1,1} is on the blue side.    \n",
    "![AND Graph](Graph-And.png)  \n",
    "\n",
    "You can try and do the same with OR, NOT, and XOR but since we don't use the perceptron rule in ANN I won't cover them here. But, there is plenty of information out there about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Stochastic Gradient Descent**  \n",
    "Since this can get pretty deep in math pretty quickly I will keep this to a high level and let you go out and get as deep into this as you want.  \n",
    "\n",
    "Stochastic gradient descent (SGD) is a more robust algorithm that will be able to separate the input but not limited to a straight line (threshold) like the perceptron rule. Instead of looking for TRUE/FALSE you are trying to find the output that matches your answer. Just know that gradient descent does converge to a local optimum and if you actually want to learn anything about machine learning of reinforcement learning you should give yourself some time and dig into SGD.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptron vs Stochastic Gradient Descent**  \n",
    "Just to show the difference I am going to compare the 2 learning rules.  \n",
    "Perceptron:  $$\\Delta W_i = \\gamma(y-y')X_i$$  \n",
    "Stochastic Gradient Descent: $$\\Delta W_i = \\gamma(y-a)X_i$$  where $$a = \\sum X_iW_i$$  \n",
    "But, with perceptron you try and find 1 or 0 because you are trying to linearly separate them. With SGD you are trying to minimze the error. Also, if you care about the math, $y'$ is non differentiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Artificial Neural Network Layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a diagram from Wikipedia: ![Image of ANN Layer](https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/200px-Colored_neural_network.svg.png)  \n",
    "\n",
    "With an ANN you have an input layer (in the cart pole it would be the 4 observations) and that layer connects to your hidden layer. There can be multiple hidden layers but the more layers the slower the training and the more chances you have of overfitting your training data. After all the hidden layer processing you have the output layer (in the cart pole this would be you Left/Right action). \n",
    "\n",
    "One thing to notice is the connections between neurons. It isn't 1 to 1 across layers. Each neuron will connect with each neuron in the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation**  \n",
    "Backpropagation is the process that is used to update the values of the neurons inside the ANN. During training, you will compare your output with the correct answer and an error will be assigned. That error will then be distributed back through each of the layers by modifying the weights of each neuron. Upon enough training you will start to find the optimal model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't have any demos in this section because we will cover this in the next 2 notebooks.  \n",
    "\n",
    "The internet is your friend if you want to go crazy and dig into backpropagation, gradient descent, etc. Hopefully, when you are all done you can at least understand enough to hold your own and do your own experiments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
