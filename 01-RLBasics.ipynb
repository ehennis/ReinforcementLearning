{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Reinforcement Learning Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In psychology, decision-making is regarded as the cognitive process resulting in the selection of a belief. In economics, it is the act of deciding on matters of the economy given certain indicators. In neuroscience, there are studies that show various brain regions work together during the decision-making process. These studies are expanded to try and figure out addictions and other self-control related items. In statistics, there are statistical decisions which are made on the basis of observations of a phenomenon that obeys probabilistic laws that are not completely known. \n",
    "\n",
    "All of this is to say that making decisions are all over every part of our lives. We take in inputs, do some processing, and output a decision. In some situations, we have a full working model of our environment and other times we have to use probabilities. If we can take all of these areas of study, we can attempt to recreate how humans make decisions and transfer that to artificial intelligence. \n",
    "  \n",
    "In reinforcement learning, we do an action in a certain state in order to move to another state and recieve some reward. We determine the ideal action by training. During training we either explore or exploit our actions. Over enough training data we will have a pretty good idea about our world. But, we won't have the full model of our environment.  \n",
    "  \n",
    "In our lives we do this all the time. It is called learning. For example, I am sitting at a desk with a glass of water next to me (state). I want to take the cup (action) and drink it (reward). Our entire life is the training data. When we were younger and didn't know anything we would try random actions (explore). As we got older and learned a few things we did actions that accomplished our goals (exploit). \n",
    "\n",
    "In this course, I will work through one type of decision-making algorithm called Q-learning. Q-learning is a type of reinforcement learning where we are given rewards for actions in an attempt to learn a policy to navigate an environment. In a Markov Decision Process (MDP), Q-learning will find an optimal policy given infinite exploration.\n",
    "\n",
    "After Q-learning, I will go through some enhancements. The first is Double Q-learning which was created by Hado van Hasselt in 2010. Double Q-learning uses a second “off-policy” to evaluation the next action. This second policy is used because in a noisy environment the action value can be overestimated and slow learning.\n",
    "\n",
    "After Double Q-learning, I will cover Deep Q-network (DQN) which was created by Google DeepMind 2015. DQN uses a neural network for the internal Q table. This allows a much larger (even continuous) environment.\n",
    "\n",
    "Finally, I will cover Double DQN which was created by Google DeepMind in 2016. It uses DQN but adds a second “off-policy” network to evaluate the next action.\n",
    "\n",
    "Welcome to the ride!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov Decision Process (MDP) provides a mathematical framework for modeling decision-making. It is a discrete time (distinct points in time) stochastic (randomly determined) process.\n",
    "\n",
    "MDPs are made up of 4 parts:  \n",
    "S: Finite set of states (Ex: s<sub>1</sub>, s<sub>2</sub> ... s<sub>N</sub>)  \n",
    "A: Finite set of actions (Ex: North, South, East, West)  \n",
    "P<sub>a</sub>(s,s'): Probability that action a in state s at time t will lead to state s' at time t + 1  \n",
    "R<sub>a</sub>(s,s'): Immediate reward received after moving from state s to state s' by action a  \n",
    "\n",
    "Some places include $\\gamma$ but I don't like it because it is a independant of the others.  \n",
    "$\\gamma$: The discount factor between 0 (inclusive) and 1 (exclusive). This determines how much you want to give the future path credit. If you think that the future reward is as important as the current reward you would set this to 0.99999. If you don't care about the future rewards you would set this to 0 and you only care about the current reward. Both of these are bad ideas. You want to find a place in the middle that will take into account expected future rewards but not too much. Some math to think about, if your discount factor is 0.8 and after 5 steps you get a reward of 4 the present value of that reward is $0.8^4 * 5$ or ~2.\n",
    "\n",
    "An MDP is a collection of states that each have a selection of actions associated with them. With each action comes a reward (can be 0). The solution to an MDP is the policy ($\\pi$). That policy will determine the optimal action to take at each state to maximize the reward. Typically, in an MDP only the current state matters. You want the optimal policy for that state based on the available actions.  \n",
    "  \n",
    "I don't want to get too technical but I do need to give the equation that you would use to find the optimal policy. It is the Bellman Equation. At the end of this section I recommend you research it some more if you want to get in the weeds. You will also find a man that has shaped where we are today. The following equation is finding the expected reward at state s following some fixed policy $\\pi$:  \n",
    "\n",
    "$$V^\\pi(s) = R(s,\\pi(s)) + \\gamma \\sum P(s'|s, \\pi(s))V^\\pi(s')$$\n",
    "Lets break these down:  \n",
    "$R(s,\\pi(s))$: This is the reward at state S using the expected policy.  \n",
    "$\\gamma$: This is the discount factor for all future rewards.  \n",
    "$\\sum$: This is the sum of the probabilities of each action mutliplied by its expected value.  \n",
    "$P(s'|s, \\pi(s))$:  Probability of going from state s to state s' using the expected policy of s  \n",
    "$V^\\pi(s')$: Expected value of state s'. Isn't recursion wonderful??  \n",
    "  \n",
    "Now, to find the OPTIMAL policy you will need to find the action that gives the highest reward.  \n",
    "\n",
    "$$V^*(s) = max_a \\{ R(s,a) + \\gamma \\sum P(s'|s, a)V^*(s') \\}$$\n",
    "The main difference here is that we take the $max_a$ value at each state. This will assume that you are taking the best action to maximized your reward.  \n",
    "\n",
    "A real world example would be an inventory control system. Your states would be the amount of items you have in stock. Your actions would be the amount to order. The discrete time would be the days of the month. The reward would be the profit.  \n",
    "\n",
    "A major drawback of MDPs is called the \"Curse of Dimensionality\". This states that the more states/actions you have the more computational difficult it is to solve.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first question of the notebook, I will give a quick example of a discrete process MDP. I will ask to see if you can put the definitions above into practice.\n",
    "\n",
    "**Question 1**: Given the following deterministic process (you select North you will move North) MDP, what is the optimal policy (path with the most points)?  \n",
    "  \n",
    "*Notes*:  \n",
    "  * The number in the box is the reward  \n",
    "  * Assume there is a negative time reward AKA you can't just sit on a single cell and collect rewards  \n",
    "  * Once you hit the end you are done. (Absorbing state)  \n",
    "  * S is the starting point  \n",
    "  * F is the ending point  \n",
    "  * Use N for North, E for East, S for South, and W for West  \n",
    "  * Pass the directions as a single string. Ex: NESWN will make a cirle  \n",
    "  \n",
    "  \n",
    "\n",
    "| | | |\n",
    "|----------|----------|---------|\n",
    "|S|1|1|\n",
    "|1 |0|1|  \n",
    "|-1|-1|0|  \n",
    "|0 |0|F|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect. Remember to collect as many rewards in as few passes as possible\n"
     ]
    }
   ],
   "source": [
    "# Code up an MDP. If I get frisky maybe the inventory control system above\n",
    "from basic import MDPQuestion1 #Import solution file\n",
    "MDPQuestion1('???') #Enter string of directions (Ex: \"NNSS\" to move North, North, South, and South)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2 through 7 will attempt to firm up your knowledge of the parts of an MDP. Just remember that (0,0) is the starting state and (1,0) is the state to the right of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect. Count the spaces. You have a 3x4 grid\n",
      "Incorrect. How many directions can you move?\n",
      "Incorrect. Where do you end up if you move right?\n",
      "Correct\n",
      "  That action runs you into a wall and will not move to the state to the east\n",
      "Incorrect. What is the reward value of cell (2,0)?\n",
      "Incorrect. What is the reward value of cell (0,2)?\n"
     ]
    }
   ],
   "source": [
    "from basic import MDPQuestion2, MDPQuestion3, MDPQuestion4, MDPQuestion5, MDPQuestion6, MDPQuestion7 #Import solution file\n",
    "#How many states are in the above MDP?\n",
    "MDPQuestion2(0) #Enter an integer value\n",
    "#How many actions are in the above MDP\n",
    "MDPQuestion3(0) #Enter an integer value\n",
    "#What is the probability of using action E at state (1,0) to state (2,0)? P[ (1,0), (2,0) ]?\n",
    "MDPQuestion4(0) #Use 0 for 0% or 0.5 for 50% or 1 for 100%\n",
    "#What is the probability of using action N at state (1,0) to state (2,0)?\n",
    "MDPQuestion5(0) #Use 0 for 0% or 0.5 for 50% or 1 for 100%\n",
    "#What is the reward for moving from state (1,0) to state (2,0)? R[ (1,0), (2,0) ]\n",
    "MDPQuestion6(0) #Enter an integer value\n",
    "#What is the reward for moving from state (0,1) to state (0,2)? R[ (0,1), (0,2) ]\n",
    "MDPQuestion7(0) #Enter an integer value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could have made this more complex using a stochastic process but I don't want to muddy up the basics. We will talk about that later when we get into q-learning. I also left off the discount factor for the same reason.  \n",
    "  \n",
    "**To get more technical information you should research**\n",
    " * Bellman Equation\n",
    " * Sequence of rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value iteration can be used instead of the optimal policy ($\\pi$). Instead of finding $\\pi$ we find V(s). V(s) is the maximum value at state s.  \n",
    "\n",
    "The value function is based on the Bellman Equation but instead of finding the policy you replace it with finding value. The equation is as follows:  \n",
    "$$V_t(s_t) = max_a \\{ R(s_t,a_t) + E V_{t+1}(s_{t+1} \\}$$\n",
    "Lets break this down:  \n",
    "$V_t(s_t)$: This is the maximum expected value in state s at time t  \n",
    "$max_a$: This chooses action a that maximized the value  \n",
    "$R(s_t,a_t)$: This is the reward at state s for action a at time t  \n",
    "$E V_{t+1}(s_{t+1} \\}$: This is the expected remaining rewards  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Here is a 3x3 grid. I am going to walk through a few iterations to firm up your understanding. To keep this simple I am NOT going to have stochasic actions. If you want to move N you will move N. But, if I was, you would need to ADD the expected rewards of the other actions together. We will have a discount factor ($\\gamma$) of 0.9.  \n",
    "\n",
    "**Iteration 1**:  \n",
    "\n",
    "| | | |  \n",
    "|----------|----------|---------|  \n",
    "|0|0|0|\n",
    "|0|10|0|  \n",
    "|0|0|0|  \n",
    "  \n",
    "**Iteration 2**:  \n",
    "  \n",
    "Staring with cell (0,0): \n",
    "We have the reward (0) for the cell. We now need to find the expected value of each move:  \n",
    "Action N: 0.9 \\* 0 = 0  \n",
    "Action E: 0.9 \\* 0 = 0  \n",
    "Action S: 0.9 \\* 0 = 0  \n",
    "Action W: 0.9 \\* 0 = 0  \n",
    "Since all of the expected values are 0 we can choose any one of them as the maximum expected value. So, cell (0,0) stays at 0.  \n",
    "  \n",
    "To get to an interesting cell, move east to cell (1,0). This one has a reward for going south. We need to take that into account when we see what our reward is but you also need to remember the discount factor.  \n",
    "Action N: 0.9 \\* 0 = 0  \n",
    "Action E: 0.9 \\* 0 = 0  \n",
    "Action S: 0.9 \\* 10 = 9 <--Max value  \n",
    "Action W: 0.9 \\* 0 = 0  \n",
    "We can see that moving S will gain us 9 reward. We add that to our current reward of 0 to get 9.  \n",
    "    \n",
    "You can follow this through and see that any space that can get to the center space will have a reward of 9.   \n",
    "\n",
    "| | | |  \n",
    "|----------|----------|---------|  \n",
    "|0|9|0|\n",
    "|9|10|9|  \n",
    "|0|9|0|  \n",
    "\n",
    "*Side Note*: If this was stochastic I would need to work with the probabilities and add their expected values together. Here is an example of using 70% of the time you get your move and 10% each other direction.  \n",
    "\n",
    "This is how you would have done cell (1,0).    \n",
    "We know that going S would get the best reward but we have to account for the slips.\n",
    "Probability of going S (0.7) times the reward (10). Then you would need to calculate the other directions.  \n",
    "Going South: 0.7 \\* (0.9 \\* 10) +  \n",
    "Slip North: 0.1 \\* (0.9 \\* 0) +  \n",
    "Slip East: 0.1 \\* (0.9 \\* 0) +  \n",
    "Slip West: 0.1 \\* (0.9 \\* 0)  \n",
    "(6.3 + 0 + 0 + 0) = 6.3  \n",
    "\n",
    "Now, let say that going E would give you a reward of 5 it would break down like this:  \n",
    "(0.7 \\* (0.9 \\* 10)) + (0.1 \\* (0.9 \\* 0)) + (0.1 \\* (0.9 \\* 5)) + (0.1 \\* (0.9 * 0))  \n",
    "6.3 + 0 + 0.45 + 0 = 6.75\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iteration 3**:  \n",
    "For iteration 3 I am going to work through some problems in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect: You need the current reward (0) and then add that to the discount factor (0.9) times the future reward (9)\n"
     ]
    }
   ],
   "source": [
    "from basic import VIQuestion1\n",
    "# Starting at (0,0) what is the maximum expected value?\n",
    "VIQuestion1(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect: You need the current reward (9) and then add that to the discount factor (0.9) times the future reward (10)\n"
     ]
    }
   ],
   "source": [
    "from basic import VIQuestion2\n",
    "# Starting at (1,0) what is the maximum expected value?\n",
    "VIQuestion2(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to go further you can code this up and see how $\\gamma$ changes your iterations. You can also add in the stochastic actions throughout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy iteration manipulate the policy directly instead of trying to find it through the value function. Most likely, this is solved by using linear equations. Since the is a whole beast unto itself I won't go into them here but it is something that you can research more.  \n",
    "  \n",
    "Using the linear equations to find the infinite discounted reward for each state using the current policy. We then change the first action taken and rerun the equations. If the policy is better we use that new action. This step is ensured to strictly improve the performance of the policy. If we get to a point where no more improvements are possible, we have the gauranteed optimal policy.  \n",
    "  \n",
    "One thing to note, there are at most $Actions^{States}$ distinct possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic and Stochastic Movements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have touched on this already but there are 2 types of movements in MDPs. The first are deterministic. These are the movements where you say to go one direction and it actually goes that direction. This is fantastic when working problems but not realistic in the real world. The second type is stochastic. This is where you choose to go one direction and it only happens X% of the time. This is typical in the real world."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
